---
title: Non-local
author:Joker
mathjax: true
type: categories
categories: paper-note
---

### Non-local Neural Networks

[论文链接](http://cn.arxiv.org/pdf/1711.07971.pdf)

#### 关键思想

​	在这篇文章中，作者受到传统的非局部均值方法（non-local means）的启发，提出了一种非局部模块（non-local block）的网络结构，用于获取视屏和图像的长程依赖关系（long-range dependencies）。考虑不同位置上特征的pairwise responese，添加这些非局部操作，也相当于对特征图进行一种降噪操作，消除feature map中的噪声。

#### 主要实现

​	non-local描述如下：
$$
y_i=\frac{1}{C(x)}\sum_{\forall j}f(x_i,x_j)g(x_j)
$$

​	$i$ 表示输出的位置，$j$ 遍历所有可能的位置，$f$ 代表相似度的函数，$g $ 是输入缩放函数，$C( x )$ 是归一化因子，$y$ 是与输入信号$x$ 相同大小的输出信号。

​	由于所有的位置信息($\forall j$ )在运算中都被涉及到了，所以这个公式是带有“非局部”的信息，并且由于在结构中使用的是$1*1*1$ 的卷积，能够使这个结构支持不同尺寸的输入，同时保持相同的输出。而传统的fc层需要固定的输入输出，同时丢失了其相对位置信息。一个non-local block 能很容易的与卷积/循环层（convolutional/recurrent layers）结合使用，并且能添加到网络结构的前面部分，而不需要像fc层一样放在网络结构的最后，这种差异可以设计出更加具有层次性的网络，能够结合non-local 与 local 信息。结构图如下所示：

![non-local-block](https://raw.githubusercontent.com/Joker1994k/pic/master/non-local%20block.PNG)

从结构图可以看出，定义的non-local块为

$$
z_i=W_zy_i +x_i
$$


是一个残差的结构，可以作为网络中的小结构插入到任意的卷积层之后，也可以比较简单的添加到各个预训练模型之中。

#### 总结

​	作者在视屏分类，物体检测，物体实例分割，姿态预测任务上做了许多对照试验实验，都有一定的提升。作者发现将non-local block放在网络的浅层效果比放在高层的效果好（高层网络丢失的信息多，位置之间的联系模糊），并且对于对于主干网络较浅的网络，加深non-local block能够提升性能。对于较大较深的网络无论是增加NL block还是继续加深主干网络的深度都很难再提升性能。

​	non-local block的方法能与任意的架构相结合，是个适用范围比较广的方法。看non-local block 的结构图，通过对位置信息的一个加权得到处理后的feature map，对判断其关键作用的位置投入更大的权重，感觉是和attention结构相似。




